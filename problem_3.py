# -*- coding: utf-8 -*-
"""Problem_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1JjcJsaN4YnFMf1QSh0doz9e5M--uXg

# 1. Data preparation
"""

import torch
import torchtext
import requests
import tarfile
from torch import Tensor, nn
from torch.nn import Transformer
from torchtext.data.metrics import bleu_score
import math
import time
torch.manual_seed(42)

# Constants
EPOCH = 30
BATCHSIZE = 16
LR = 0.0001
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
MODELNAME = 'transformers.model'
PAD_IDX = 1
SRC_VOCAB_SIZE = 0
TGT_VOCAB_SIZE = 0
EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 512
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3
LSTM_HIDDEN_SIZE = 256  # LSTM hidden layer size
LSTM_NUM_LAYERS = 2     # Number of LSTM layers

def iwslt15(train_test):
    url = "https://github.com/stefan-it/nmt-en-vi/raw/master/data/"
    r = requests.get(url + train_test + "-en-vi.tgz")
    filename = train_test + "-en-vi.tar.gz"
    with open(filename, 'wb') as f:
        f.write(r.content)
    tarfile.open(filename, 'r:gz').extractall("iwslt15")

iwslt15("train")
iwslt15("test-2013")

def read_data(filepath):
    with open(filepath) as f:
        return [line.split() for line in f]

train_en = read_data("iwslt15/train.en")
train_vi = read_data("iwslt15/train.vi")
test_en = read_data("iwslt15/tst2013.en")
test_vi = read_data("iwslt15/tst2013.vi")

"""## Make vocab"""

def make_vocab(train_data, min_freq):
    vocab = {}
    for tokenlist in train_data:
        for token in tokenlist:
            vocab[token] = vocab.get(token, 0) + 1
    vocablist = [('<unk>', 0), ('<pad>', 0), ('<cls>', 0), ('<eos>', 0)]
    vocabidx = {token: idx for idx, (token, freq) in enumerate(vocablist)}
    for token, freq in vocab.items():
        if freq >= min_freq:
            idx = len(vocablist)
            vocablist.append((token, freq))
            vocabidx[token] = idx
    return vocablist, vocabidx

vocablist_en, vocabidx_en = make_vocab(train_en, 3)
vocablist_vi, vocabidx_vi = make_vocab(train_vi, 3)

"""## Preprocess"""

def preprocess(data, vocabidx):
    return [['<cls>'] + [token if token in vocabidx else '<unk>' for token in tokenlist] + ['<eos>'] for tokenlist in data]

train_en_prep = preprocess(train_en, vocabidx_en)
train_vi_prep = preprocess(train_vi, vocabidx_vi)
test_en_prep = preprocess(test_en, vocabidx_en)

train_data = sorted(list(zip(train_en_prep, train_vi_prep)), key=lambda x: (len(x[0]), len(x[1])))
test_data = list(zip(test_en_prep, test_en, test_vi))

"""## Make Batch"""

def make_batch(data):
    batches = []
    batch_en, batch_vi = [], []
    for en, vi in data:
        batch_en.append(en)
        batch_vi.append(vi)
        if len(batch_en) >= BATCHSIZE:
            batches.append((batch_en, batch_vi))
            batch_en, batch_vi = [], []
    if batch_en:
        batches.append((batch_en, batch_vi))
    return batches

train_data = make_batch(train_data)

"""## Padding Batch"""

def padding_batch(batch):
    maxlen = max(len(x) for x in batch)
    for tokenlist in batch:
        tokenlist.extend(['<pad>'] * (maxlen - len(tokenlist)))

def padding(batches):
    for batch_en, batch_vi in batches:
        padding_batch(batch_en)
        padding_batch(batch_vi)

padding(train_data)

train_data = [([[vocabidx_en[token] for token in tokenlist] for tokenlist in batch_en],
               [[vocabidx_vi[token] for token in tokenlist] for tokenlist in batch_vi])
              for batch_en, batch_vi in train_data]
test_data = [([vocabidx_en[token] for token in enprep], en, vi) for enprep, en, vi in test_data]

"""# 2. Prepare For Model

## Positional Encoding
"""

class PositionalEncoding(nn.Module):
    def __init__(self, emb_size, dropout=0.01, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, emb_size)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-math.log(10000.0) / emb_size))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

"""## Token Embedding"""

class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size
        nn.init.normal_(self.embedding.weight, mean=0, std=emb_size ** -0.5)

    def forward(self, tokens):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

"""## Seq2Seq Transformer"""

class Seq2SeqTransformer(nn.Module):
    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=2048, dropout=0.01):
        super(Seq2SeqTransformer, self).__init__()
        self.transformer = Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)
        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(emb_size, dropout)

        self._reset_parameters()

    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))
        memory = self.transformer.encoder(src_emb, src_mask, src_key_padding_mask=src_padding_mask)
        outs = self.transformer.decoder(tgt_emb, memory, tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask)
        return self.generator(outs)

    def encode(self, src, src_mask):
        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)

    def decode(self, tgt, memory, tgt_mask):
        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)

"""# 3. Model Transformer"""

SRC_VOCAB_SIZE = len(vocablist_en)
TGT_VOCAB_SIZE = len(vocablist_vi)

model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)
for p in model.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)

loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)

def generate_square_subsequent_mask(sz):
    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

def create_mask(src, tgt):
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt.shape[0]
    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)
    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)
    src_padding_mask = (src == vocabidx_en['<pad>']).transpose(0, 1)
    tgt_padding_mask = (tgt == vocabidx_vi['<pad>']).transpose(0, 1)
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask

"""# 4. Train Model"""

def train():
    model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)
    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

    model.train()
    for epoch in range(EPOCH):
        start = time.time()
        total_loss = 0
        for en, vi in train_data:
            en = torch.tensor(en, dtype=torch.int64).transpose(0, 1).to(DEVICE)
            vi = torch.tensor(vi, dtype=torch.int64).transpose(0, 1).to(DEVICE)
            tgt_input = vi[:-1, :]
            optimizer.zero_grad()
            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(en, tgt_input)
            y = model(en, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)
            tgt_out = vi[1:, :]
            loss = loss_fn(y.reshape(-1, y.shape[-1]), tgt_out.reshape(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        average_loss = total_loss / len(train_data)
        end = time.time()
        print(f'Epoch {epoch+1}, Loss: {average_loss:.4f}, Time: {end-start:.2f}s')
    torch.save(model.state_dict(), MODELNAME)

train()

"""# 5. Test Model"""

def evaluate(model, src, max_len, start_symbol):
    src_mask = torch.zeros((src.shape[0], src.shape[0]), device=DEVICE).type(torch.bool)
    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for _ in range(max_len - 1):
        tgt_mask = generate_square_subsequent_mask(ys.size(0)).type(torch.bool).to(DEVICE)
        out = model.decode(ys, memory, tgt_mask)
        prob = model.generator(out.transpose(0, 1)[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        if next_word == vocabidx_vi['<eos>']:
            break
    return [vocablist_vi[idx][0] for idx in ys.squeeze().tolist()[1:]]

def test():
    model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)
    model.load_state_dict(torch.load(MODELNAME))
    model.eval()
    ref, pred = [], []
    for enprep, _, vi in test_data:
        input_tensor = torch.tensor([enprep], dtype=torch.int64).transpose(0, 1).to(DEVICE).view(-1, 1)
        pred_vi = evaluate(model, input_tensor, 50, vocabidx_en['<cls>'])
        ref.append([vi])
        pred.append(pred_vi)
    bleu = torchtext.data.metrics.bleu_score(pred, ref)
    print(f"BLEU score: {bleu:.4f}")

test()